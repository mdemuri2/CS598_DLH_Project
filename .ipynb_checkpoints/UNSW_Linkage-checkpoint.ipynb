{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source of Code:\n",
    "This code reproduces the results of the “Statistical supervised meta-ensemble algorithm for medical record linkage” paper. The vast majority of this code was sourced from the original paper’s GitHub repository. The original code has been slightly modified and amended.\n",
    "\n",
    "K. Vo, J. Jonnagaddala and S.-T. Liaw, \"Medical-Record-Linkage-Ensemble,\" 16 February 2019. [Online]. Available: https://github.com/ePBRN/Medical-Record-Linkage-Ensemble/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Source: \n",
    "K. Vo, J. Jonnagaddala and S.-T. Liaw, \"Medical-Record-Linkage-Ensemble,\" 16 February 2019. [Online]. \n",
    "Available: https://github.com/ePBRN/Medical-Record-Linkage-Ensemble/.\n",
    "'''\n",
    "import recordlinkage as rl, pandas as pd, numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from recordlinkage.preprocessing import phonetic\n",
    "from numpy.random import choice\n",
    "import collections, numpy\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from math import comb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 FEBRL Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Source: \n",
    "K. Vo, J. Jonnagaddala and S.-T. Liaw, \"Medical-Record-Linkage-Ensemble,\" 16 February 2019. [Online]. \n",
    "Available: https://github.com/ePBRN/Medical-Record-Linkage-Ensemble/.\n",
    "'''\n",
    "def generate_true_links(df): \n",
    "    # although the match_id column is included in the original df to imply the true links,\n",
    "    # this function will create the true_link object identical to the true_links properties\n",
    "    # of recordlinkage toolkit, in order to exploit \"Compare.compute()\" from that toolkit\n",
    "    # in extract_function() for extracting features quicker.\n",
    "    # This process should be deprecated in the future release of the UNSW toolkit.\n",
    "    df[\"rec_id\"] = df.index.values.tolist()\n",
    "    indices_1 = []\n",
    "    indices_2 = []\n",
    "    processed = 0\n",
    "    for match_id in df[\"match_id\"].unique():\n",
    "        if match_id != -1:    \n",
    "            processed = processed + 1\n",
    "            # print(\"In routine generate_true_links(), count =\", processed)\n",
    "            # clear_output(wait=True)\n",
    "            linkages = df.loc[df['match_id'] == match_id]\n",
    "            for j in range(len(linkages)-1):\n",
    "                for k in range(j+1, len(linkages)):\n",
    "                    indices_1 = indices_1 + [linkages.iloc[j][\"rec_id\"]]\n",
    "                    indices_2 = indices_2 + [linkages.iloc[k][\"rec_id\"]]    \n",
    "    links = pd.MultiIndex.from_arrays([indices_1,indices_2])\n",
    "    return links\n",
    "\n",
    "def generate_false_links(df, size):\n",
    "    # A counterpart of generate_true_links(), with the purpose to generate random false pairs\n",
    "    # for training. The number of false pairs in specified as \"size\".\n",
    "    df[\"rec_id\"] = df.index.values.tolist()\n",
    "    indices_1 = []\n",
    "    indices_2 = []\n",
    "    unique_match_id = df[\"match_id\"].unique()\n",
    "    for j in range(size):\n",
    "            false_pair_ids = choice(unique_match_id, 2)\n",
    "            candidate_1_cluster = df.loc[df['match_id'] == false_pair_ids[0]]\n",
    "            candidate_1 = candidate_1_cluster.iloc[choice(range(len(candidate_1_cluster)))]\n",
    "            candidate_2_cluster = df.loc[df['match_id'] == false_pair_ids[1]]\n",
    "            candidate_2 = candidate_2_cluster.iloc[choice(range(len(candidate_2_cluster)))]    \n",
    "            indices_1 = indices_1 + [candidate_1[\"rec_id\"]]\n",
    "            indices_2 = indices_2 + [candidate_2[\"rec_id\"]]  \n",
    "    links = pd.MultiIndex.from_arrays([indices_1,indices_2])\n",
    "    return links\n",
    "\n",
    "def swap_fields_flag(f11, f12, f21, f22):\n",
    "    return int((f11 == f22) and (f12 == f21))\n",
    "\n",
    "def extract_features(df, links):\n",
    "    c = rl.Compare()\n",
    "    c.string('given_name', 'given_name', method='jarowinkler', label='y_name')\n",
    "    c.string('given_name_soundex', 'given_name_soundex', method='jarowinkler', label='y_name_soundex')\n",
    "    c.string('given_name_nysiis', 'given_name_nysiis', method='jarowinkler', label='y_name_nysiis')\n",
    "    c.string('surname', 'surname', method='jarowinkler', label='y_surname')\n",
    "    c.string('surname_soundex', 'surname_soundex', method='jarowinkler', label='y_surname_soundex')\n",
    "    c.string('surname_nysiis', 'surname_nysiis', method='jarowinkler', label='y_surname_nysiis')\n",
    "    c.exact('street_number', 'street_number', label='y_street_number')\n",
    "    c.string('address_1', 'address_1', method='levenshtein', threshold=0.7, label='y_address1')\n",
    "    c.string('address_2', 'address_2', method='levenshtein', threshold=0.7, label='y_address2')\n",
    "    c.exact('postcode', 'postcode', label='y_postcode')\n",
    "    c.exact('day', 'day', label='y_day')\n",
    "    c.exact('month', 'month', label='y_month')\n",
    "    c.exact('year', 'year', label='y_year')\n",
    "        \n",
    "    # Build features\n",
    "    feature_vectors = c.compute(links, df, df)\n",
    "    return feature_vectors\n",
    "\n",
    "def generate_train_X_y(df):\n",
    "    # This routine is to generate the feature vector X and the corresponding labels y\n",
    "    # with exactly equal number of samples for both classes to train the classifier.\n",
    "    pos = extract_features(df, train_true_links)\n",
    "    train_false_links = generate_false_links(df, len(train_true_links))    \n",
    "    neg = extract_features(df, train_false_links)\n",
    "    X = pos.values.tolist() + neg.values.tolist()\n",
    "    y = [1]*len(pos)+[0]*len(neg)\n",
    "    X, y = shuffle(X, y, random_state=0)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "def train_model(modeltype, modelparam, train_vectors, train_labels, modeltype_2):\n",
    "    if modeltype == 'svm': # Support Vector Machine\n",
    "        model = svm.SVC(C = modelparam, kernel = modeltype_2)\n",
    "        model.fit(train_vectors, train_labels) \n",
    "    elif modeltype == 'lg': # Logistic Regression\n",
    "        model = LogisticRegression(C=modelparam, penalty = modeltype_2,class_weight=None, dual=False, fit_intercept=True, \n",
    "                                   intercept_scaling=1, max_iter=5000, multi_class='ovr', \n",
    "                                   n_jobs=1, random_state=None)\n",
    "        model.fit(train_vectors, train_labels)\n",
    "    elif modeltype == 'nb': # Naive Bayes\n",
    "        model = GaussianNB()\n",
    "        model.fit(train_vectors, train_labels)\n",
    "    elif modeltype == 'nn': # Neural Network\n",
    "        model = MLPClassifier(solver='lbfgs', alpha=modelparam, hidden_layer_sizes=(256, ), \n",
    "                              activation = modeltype_2,random_state=None, batch_size='auto', \n",
    "                              learning_rate='constant',  learning_rate_init=0.001, \n",
    "                              power_t=0.5, max_iter=10000, shuffle=True, \n",
    "                              tol=0.0001, verbose=False, warm_start=False, momentum=0.9, \n",
    "                              nesterovs_momentum=True, early_stopping=False, \n",
    "                              validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "        model.fit(train_vectors, train_labels)\n",
    "    return model\n",
    "\n",
    "def classify(model, test_vectors):\n",
    "    result = model.predict(test_vectors)\n",
    "    return result\n",
    "\n",
    "    \n",
    "def evaluation(test_labels, result):\n",
    "    true_pos = np.logical_and(test_labels, result)\n",
    "    count_true_pos = np.sum(true_pos)\n",
    "    true_neg = np.logical_and(np.logical_not(test_labels),np.logical_not(result))\n",
    "    count_true_neg = np.sum(true_neg)\n",
    "    false_pos = np.logical_and(np.logical_not(test_labels), result)\n",
    "    count_false_pos = np.sum(false_pos)\n",
    "    false_neg = np.logical_and(test_labels,np.logical_not(result))\n",
    "    count_false_neg = np.sum(false_neg)\n",
    "    precision = count_true_pos/(count_true_pos+count_false_pos)\n",
    "    sensitivity = count_true_pos/(count_true_pos+count_false_neg) # sensitivity = recall\n",
    "    confusion_matrix = [count_true_pos, count_false_pos, count_false_neg, count_true_neg]\n",
    "    no_links_found = np.count_nonzero(result)\n",
    "    no_false = count_false_pos + count_false_neg\n",
    "    Fscore = 2*precision*sensitivity/(precision+sensitivity)\n",
    "    metrics_result = {'no_false':no_false, 'confusion_matrix':confusion_matrix ,'precision':precision,\n",
    "                     'sensitivity':sensitivity ,'no_links':no_links_found, 'F-score': Fscore}\n",
    "    return metrics_result\n",
    "\n",
    "def blocking_performance(candidates, true_links, df):\n",
    "    count = 0\n",
    "    for candi in candidates:\n",
    "        if df.loc[candi[0]][\"match_id\"]==df.loc[candi[1]][\"match_id\"]:\n",
    "            count = count + 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Source: \n",
    "K. Vo, J. Jonnagaddala and S.-T. Liaw, \"Medical-Record-Linkage-Ensemble,\" 16 February 2019. [Online]. \n",
    "Available: https://github.com/ePBRN/Medical-Record-Linkage-Ensemble/.\n",
    "'''\n",
    "trainset = 'febrl3_UNSW'\n",
    "testset = 'febrl4_UNSW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import train set...\n",
      "Train set size: 5000 , number of matched pairs:  1165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/.venv/lib/python3.9/site-packages/recordlinkage/preprocessing/encoding.py:80: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  s = s.str.replace(r\"[\\-\\_\\s]\", \"\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/.venv/lib/python3.9/site-packages/recordlinkage/preprocessing/encoding.py:80: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  s = s.str.replace(r\"[\\-\\_\\s]\", \"\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/.venv/lib/python3.9/site-packages/recordlinkage/preprocessing/encoding.py:80: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  s = s.str.replace(r\"[\\-\\_\\s]\", \"\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/.venv/lib/python3.9/site-packages/recordlinkage/preprocessing/encoding.py:80: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  s = s.str.replace(r\"[\\-\\_\\s]\", \"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished building X_train, y_train\n",
      "CPU times: user 834 ms, sys: 54.1 ms, total: 888 ms\n",
      "Wall time: 888 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "Source: \n",
    "K. Vo, J. Jonnagaddala and S.-T. Liaw, \"Medical-Record-Linkage-Ensemble,\" 16 February 2019. [Online]. \n",
    "Available: https://github.com/ePBRN/Medical-Record-Linkage-Ensemble/.\n",
    "'''\n",
    "## TRAIN SET CONSTRUCTION\n",
    "\n",
    "# Import\n",
    "print(\"Import train set...\")\n",
    "df_train = pd.read_csv(trainset+\".csv\", index_col = \"rec_id\")\n",
    "train_true_links = generate_true_links(df_train)\n",
    "print(\"Train set size:\", len(df_train), \", number of matched pairs: \", str(len(train_true_links)))\n",
    "\n",
    "# Preprocess train set\n",
    "df_train['postcode'] = df_train['postcode'].astype(str)\n",
    "df_train['given_name_soundex'] = phonetic(df_train['given_name'], method='soundex')\n",
    "df_train['given_name_nysiis'] = phonetic(df_train['given_name'], method='nysiis')\n",
    "df_train['surname_soundex'] = phonetic(df_train['surname'], method='soundex')\n",
    "df_train['surname_nysiis'] = phonetic(df_train['surname'], method='nysiis')\n",
    "\n",
    "# Final train feature vectors and labels\n",
    "X_train, y_train = generate_train_X_y(df_train)\n",
    "print(\"Finished building X_train, y_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 FEBRL Blocking Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import test set...\n",
      "Test set size: 10000 , number of matched pairs:  5000\n",
      "BLOCKING PERFORMANCE:\n",
      "Number of pairs of matched given_name: 154898 , detected  3287 /5000 true matched pairs, missed 1713\n",
      "Number of pairs of matched surname: 170843 , detected  3325 /5000 true matched pairs, missed 1675\n",
      "Number of pairs of matched postcode: 53197 , detected  4219 /5000 true matched pairs, missed 781\n",
      "Number of pairs of at least 1 field matched: 372073 , detected  4894 /5000 true matched pairs, missed 106\n",
      "CPU times: user 56.5 s, sys: 227 ms, total: 56.8 s\n",
      "Wall time: 56.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "Source: \n",
    "K. Vo, J. Jonnagaddala and S.-T. Liaw, \"Medical-Record-Linkage-Ensemble,\" 16 February 2019. [Online]. \n",
    "Available: https://github.com/ePBRN/Medical-Record-Linkage-Ensemble/.\n",
    "\n",
    "Code has been modified to reproduce and print Table 4 of the paper.\n",
    "'''\n",
    "# Blocking Criteria: declare non-match of all of the below fields disagree\n",
    "# Import\n",
    "print(\"Import test set...\")\n",
    "FEBRL_blocking_results = []\n",
    "df_test = pd.read_csv(testset+\".csv\", index_col = \"rec_id\")\n",
    "test_true_links = generate_true_links(df_test)\n",
    "leng_test_true_links = len(test_true_links)\n",
    "print(\"Test set size:\", len(df_test), \", number of matched pairs: \", str(leng_test_true_links))\n",
    "\n",
    "total_possible_pairs = comb(len(df_test),2)\n",
    "match_pairs = leng_test_true_links\n",
    "\n",
    "print(\"BLOCKING PERFORMANCE:\")\n",
    "blocking_fields = [\"given_name\", \"surname\", \"postcode\"]\n",
    "all_candidate_pairs = []\n",
    "for field in blocking_fields:\n",
    "    block_indexer = rl.BlockIndex(on=field)\n",
    "    candidates = block_indexer.index(df_test)\n",
    "    detects = blocking_performance(candidates, test_true_links, df_test)\n",
    "    all_candidate_pairs = candidates.union(all_candidate_pairs)\n",
    "    print(\"Number of pairs of matched \"+ field +\": \"+str(len(candidates)), \", detected \",\n",
    "         detects,'/'+ str(leng_test_true_links) + \" true matched pairs, missed \" + \n",
    "          str(leng_test_true_links-detects) )\n",
    "    # row 1\n",
    "    row = []\n",
    "    row.append(field)\n",
    "    row.append('nc')\n",
    "    nc = len(candidates)\n",
    "    row.append(nc)\n",
    "    FEBRL_blocking_results.append(row)\n",
    "    \n",
    "    # row 2 \n",
    "    row = []\n",
    "    row.append(field)\n",
    "    row.append('pc')\n",
    "    pc = round(detects/match_pairs*100.0, 2)\n",
    "    row.append(pc)\n",
    "    FEBRL_blocking_results.append(row)\n",
    "    \n",
    "    # row 3\n",
    "    row = []\n",
    "    row.append(field)\n",
    "    row.append('rr')\n",
    "    rr = round((1-(len(candidates)/1.0/total_possible_pairs))*100, 2)\n",
    "    row.append(rr)\n",
    "    FEBRL_blocking_results.append(row)\n",
    "    \n",
    "detects = blocking_performance(all_candidate_pairs, test_true_links, df_test)\n",
    "print(\"Number of pairs of at least 1 field matched: \" + str(len(all_candidate_pairs)), \", detected \",\n",
    "     detects,'/'+ str(leng_test_true_links) + \" true matched pairs, missed \" + \n",
    "          str(leng_test_true_links-detects) )\n",
    "\n",
    "#Reproducing Table 4\n",
    "# row 1\n",
    "row_all = []\n",
    "row_all.append('All')\n",
    "row_all.append('nc')\n",
    "nc = len(all_candidate_pairs)\n",
    "row_all.append(nc)\n",
    "FEBRL_blocking_results.append(row_all)\n",
    "\n",
    "# row 2\n",
    "row_all = []\n",
    "row_all.append('All')\n",
    "row_all.append('pc')\n",
    "pc = round(detects/match_pairs*100.0, 2)\n",
    "row_all.append(pc)\n",
    "FEBRL_blocking_results.append(row_all)\n",
    "\n",
    "# row 3\n",
    "row_all = []\n",
    "row_all.append('All')\n",
    "row_all.append('rr')\n",
    "rr = round((1-(len(candidates)/1.0/total_possible_pairs))*100, 2)\n",
    "row_all.append(rr)\n",
    "FEBRL_blocking_results.append(row_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 FEBRL Classification Performance Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Learners Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test set...\n",
      "Preprocess...\n",
      "Extract feature vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/.venv/lib/python3.9/site-packages/recordlinkage/preprocessing/encoding.py:80: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  s = s.str.replace(r\"[\\-\\_\\s]\", \"\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/.venv/lib/python3.9/site-packages/recordlinkage/preprocessing/encoding.py:80: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  s = s.str.replace(r\"[\\-\\_\\s]\", \"\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/.venv/lib/python3.9/site-packages/recordlinkage/preprocessing/encoding.py:80: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  s = s.str.replace(r\"[\\-\\_\\s]\", \"\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/.venv/lib/python3.9/site-packages/recordlinkage/preprocessing/encoding.py:80: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  s = s.str.replace(r\"[\\-\\_\\s]\", \"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count labels of y_test: Counter({0: 367179, 1: 4894})\n",
      "Finished building X_test, y_test\n",
      "CPU times: user 36.8 s, sys: 239 ms, total: 37.1 s\n",
      "Wall time: 37.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "Source: \n",
    "K. Vo, J. Jonnagaddala and S.-T. Liaw, \"Medical-Record-Linkage-Ensemble,\" 16 February 2019. [Online]. \n",
    "Available: https://github.com/ePBRN/Medical-Record-Linkage-Ensemble/.\n",
    "'''\n",
    "## TEST SET CONSTRUCTION\n",
    "\n",
    "# Preprocess test set\n",
    "print(\"Processing test set...\")\n",
    "print(\"Preprocess...\")\n",
    "df_test['postcode'] = df_test['postcode'].astype(str)\n",
    "df_test['given_name_soundex'] = phonetic(df_test['given_name'], method='soundex')\n",
    "df_test['given_name_nysiis'] = phonetic(df_test['given_name'], method='nysiis')\n",
    "df_test['surname_soundex'] = phonetic(df_test['surname'], method='soundex')\n",
    "df_test['surname_nysiis'] = phonetic(df_test['surname'], method='nysiis')\n",
    "\n",
    "# Test feature vectors and labels construction\n",
    "print(\"Extract feature vectors...\")\n",
    "df_X_test = extract_features(df_test, all_candidate_pairs)\n",
    "vectors = df_X_test.values.tolist()\n",
    "labels = [0]*len(vectors)\n",
    "feature_index = df_X_test.index\n",
    "for i in range(0, len(feature_index)):\n",
    "    if df_test.loc[feature_index[i][0]][\"match_id\"]==df_test.loc[feature_index[i][1]][\"match_id\"]:\n",
    "        labels[i] = 1\n",
    "X_test, y_test = shuffle(vectors, labels, random_state=0)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "print(\"Count labels of y_test:\",collections.Counter(y_test))\n",
    "print(\"Finished building X_test, y_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.91 s, sys: 1.75 s, total: 6.66 s\n",
      "Wall time: 2.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "Modifying the code provided by the authors to produce the results in Table 6 of the paper. \n",
    "Used the hyperparameters as specified by Table 5 of the paper to build the models.\n",
    "\n",
    "Source: \n",
    "K. Vo, J. Jonnagaddala and S.-T. Liaw, \"Medical-Record-Linkage-Ensemble,\" 16 February 2019. [Online]. \n",
    "Available: https://github.com/ePBRN/Medical-Record-Linkage-Ensemble/.\n",
    "'''\n",
    "\n",
    "FEBRL_classification_results = [] \n",
    "\n",
    "## BASE LEARNERS CLASSIFICATION AND EVALUATION\n",
    "################# SVM ########################\n",
    "'''\n",
    "Table 5 Hyperparameters for SVM on the FEBRL dataset\n",
    "1. Linear kernel\n",
    "2. C = 0.005\n",
    "'''\n",
    "modeltype = 'svm' # choose between 'svm', 'lg', 'nn'\n",
    "modeltype_2 = 'linear'  # 'linear' or 'rbf' for svm, 'l1' or 'l2' for lg, 'relu' or 'logistic' for nn\n",
    "modelparam = 0.005\n",
    "\n",
    "md = train_model(modeltype, modelparam, X_train, y_train, modeltype_2)\n",
    "final_result = classify(md, X_test)\n",
    "final_eval = evaluation(y_test, final_result)\n",
    "precision = final_eval['precision']\n",
    "sensitivity = final_eval['sensitivity']\n",
    "Fscore = final_eval['F-score']\n",
    "nb_false  = final_eval['no_false']\n",
    "\n",
    "row = []\n",
    "row.append('SVM')\n",
    "row.append(round(precision*100,2))\n",
    "row.append(round(sensitivity*100, 2))\n",
    "row.append(round(Fscore*100, 2))\n",
    "row.append(nb_false)\n",
    "FEBRL_classification_results.append(row)\n",
    "\n",
    "## BASE LEARNERS CLASSIFICATION AND EVALUATION\n",
    "################# NN ########################\n",
    "'''\n",
    "Table 5 Hyperparameters for NN on the FEBRL dataset\n",
    "1. ReLu activation with a = 100\n",
    "'''\n",
    "modeltype = 'nn' # choose between 'svm', 'lg', 'nn'\n",
    "modeltype_2 = 'relu'  # 'linear' or 'rbf' for svm, 'l1' or 'l2' for lg, 'relu' or 'logistic' for nn\n",
    "modelparam = 100\n",
    "\n",
    "md = train_model(modeltype, modelparam, X_train, y_train, modeltype_2)\n",
    "final_result = classify(md, X_test)\n",
    "final_eval = evaluation(y_test, final_result)\n",
    "precision = final_eval['precision']\n",
    "sensitivity = final_eval['sensitivity']\n",
    "Fscore = final_eval['F-score']\n",
    "nb_false = final_eval['no_false']\n",
    "\n",
    "row = []\n",
    "row.append('NN')\n",
    "row.append(round(precision*100, 2))\n",
    "row.append(round(sensitivity*100, 2))\n",
    "row.append(round(Fscore*100, 2))\n",
    "row.append(nb_false)\n",
    "FEBRL_classification_results.append(row)\n",
    "\n",
    "## BASE LEARNERS CLASSIFICATION AND EVALUATION\n",
    "################# LR ########################\n",
    "'''\n",
    "Table 5 Hyperparameters for NN on the FEBRL dataset\n",
    "1. Regularization I2\n",
    "2. C = 0.2\n",
    "'''\n",
    "modeltype = 'lg' # choose between 'svm', 'lg', 'nn'\n",
    "modeltype_2 = 'l2'  # 'linear' or 'rbf' for svm, 'l1' or 'l2' for lg, 'relu' or 'logistic' for nn\n",
    "modelparam = 0.2\n",
    "\n",
    "md = train_model(modeltype, modelparam, X_train, y_train, modeltype_2)\n",
    "final_result = classify(md, X_test)\n",
    "final_eval = evaluation(y_test, final_result)\n",
    "precision = final_eval['precision']\n",
    "sensitivity = final_eval['sensitivity']\n",
    "Fscore = final_eval['F-score']\n",
    "nb_false = final_eval['no_false']\n",
    "\n",
    "row = []\n",
    "row.append('LR')\n",
    "row.append(round(precision*100, 2))\n",
    "row.append(round(sensitivity*100, 2))\n",
    "row.append(round(Fscore*100, 2))\n",
    "row.append(nb_false)\n",
    "FEBRL_classification_results.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59 s, sys: 18.3 s, total: 1min 17s\n",
      "Wall time: 29.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "Modifying the code provided by the authors to produce the results in Table 6 of the paper. \n",
    "Used the hyperparameters as specified by Table 5 of the paper to build the models.\n",
    "\n",
    "Source: \n",
    "K. Vo, J. Jonnagaddala and S.-T. Liaw, \"Medical-Record-Linkage-Ensemble,\" 16 February 2019. [Online]. \n",
    "Available: https://github.com/ePBRN/Medical-Record-Linkage-Ensemble/.\n",
    "'''\n",
    "## ENSEMBLE CLASSIFICATION AND EVALUATION\n",
    "\n",
    "# print(\"BAGGING PERFORMANCE:\\n\")\n",
    "modeltypes = ['svm', 'nn', 'lg'] \n",
    "modeltypes_2 = ['linear', 'relu', 'l2']\n",
    "modelparams = [0.005, 100, 0.2]\n",
    "nFold = 10\n",
    "kf = KFold(n_splits=nFold)\n",
    "model_raw_score = [0]*3\n",
    "model_binary_score = [0]*3\n",
    "model_i = 0\n",
    "for model_i in range(3):\n",
    "    modeltype = modeltypes[model_i]\n",
    "    modeltype_2 = modeltypes_2[model_i]\n",
    "    modelparam = modelparams[model_i]\n",
    "    # print(modeltype, \"per fold:\")\n",
    "    iFold = 0\n",
    "    result_fold = [0]*nFold\n",
    "    final_eval_fold = [0]*nFold\n",
    "    for train_index, valid_index in kf.split(X_train):\n",
    "        X_train_fold = X_train[train_index]\n",
    "        y_train_fold = y_train[train_index]\n",
    "        md =  train_model(modeltype, modelparam, X_train_fold, y_train_fold, modeltype_2)\n",
    "        result_fold[iFold] = classify(md, X_test)\n",
    "        final_eval_fold[iFold] = evaluation(y_test, result_fold[iFold])\n",
    "        # print(\"Fold\", str(iFold), final_eval_fold[iFold])\n",
    "        iFold = iFold + 1\n",
    "    bagging_raw_score = np.average(result_fold, axis=0)\n",
    "    bagging_binary_score  = np.copy(bagging_raw_score)\n",
    "    bagging_binary_score[bagging_binary_score > 0.5] = 1\n",
    "    bagging_binary_score[bagging_binary_score <= 0.5] = 0\n",
    "    bagging_eval = evaluation(y_test, bagging_binary_score)\n",
    "    # print(modeltype, \"bagging:\", bagging_eval)\n",
    "    # print('')\n",
    "\n",
    "    if modeltype == 'svm':\n",
    "        row = []\n",
    "        row.append('SVM-bag')\n",
    "        row.append(round(bagging_eval['precision']*100, 2))\n",
    "        row.append(round(bagging_eval['sensitivity']*100, 2))\n",
    "        row.append(round(bagging_eval['F-score']*100, 2))\n",
    "        row.append(bagging_eval['no_false'])\n",
    "        FEBRL_classification_results.append(row)\n",
    "    elif modeltype == 'nn':\n",
    "        row = []\n",
    "        row.append('NN-bag')\n",
    "        row.append(round(bagging_eval['precision']*100, 2))\n",
    "        row.append(round(bagging_eval['sensitivity']*100, 2))\n",
    "        row.append(round(bagging_eval['F-score']*100, 2))\n",
    "        row.append(bagging_eval['no_false'])\n",
    "        FEBRL_classification_results.append(row)\n",
    "    else:\n",
    "        row = []\n",
    "        row.append('LR-bag')\n",
    "        row.append(round(bagging_eval['precision']*100, 2))\n",
    "        row.append(round(bagging_eval['sensitivity']*100, 2))\n",
    "        row.append(round(bagging_eval['F-score']*100, 2))\n",
    "        row.append(bagging_eval['no_false'])\n",
    "        FEBRL_classification_results.append(row)\n",
    "    \n",
    "    model_raw_score[model_i] = bagging_raw_score\n",
    "    model_binary_score[model_i] = bagging_binary_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STACKING PERFORMANCE:\n",
      "\n",
      "CPU times: user 20.6 ms, sys: 19.2 ms, total: 39.9 ms\n",
      "Wall time: 8.05 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "Source: \n",
    "K. Vo, J. Jonnagaddala and S.-T. Liaw, \"Medical-Record-Linkage-Ensemble,\" 16 February 2019. [Online]. \n",
    "Available: https://github.com/ePBRN/Medical-Record-Linkage-Ensemble/.\n",
    "'''\n",
    "thres = .99\n",
    "\n",
    "print(\"STACKING PERFORMANCE:\\n\")\n",
    "stack_raw_score = np.average(model_raw_score, axis=0)\n",
    "stack_binary_score = np.copy(stack_raw_score)\n",
    "stack_binary_score[stack_binary_score > thres] = 1\n",
    "stack_binary_score[stack_binary_score <= thres] = 0\n",
    "stacking_eval = evaluation(y_test, stack_binary_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = []\n",
    "row.append('Stack+Bag')\n",
    "row.append(round(stacking_eval['precision']*100, 2))\n",
    "row.append(round(stacking_eval['sensitivity']*100, 2))\n",
    "row.append(round(stacking_eval['F-score']*100, 2))\n",
    "row.append(stacking_eval['no_false'])\n",
    "FEBRL_classification_results.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 ePBRN Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Source: \n",
    "K. Vo, J. Jonnagaddala and S.-T. Liaw, \"Medical-Record-Linkage-Ensemble,\" 16 February 2019. [Online]. \n",
    "Available: https://github.com/ePBRN/Medical-Record-Linkage-Ensemble/.\n",
    "'''\n",
    "trainset = 'ePBRN_F_dup' \n",
    "testset = 'ePBRN_D_dup'\n",
    "\n",
    "import recordlinkage as rl, pandas as pd, numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from recordlinkage.preprocessing import phonetic\n",
    "from numpy.random import choice\n",
    "import collections, numpy\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "\n",
    "def generate_true_links(df): \n",
    "    # although the match_id column is included in the original df to imply the true links,\n",
    "    # this function will create the true_link object identical to the true_links properties\n",
    "    # of recordlinkage toolkit, in order to exploit \"Compare.compute()\" from that toolkit\n",
    "    # in extract_function() for extracting features quicker.\n",
    "    # This process should be deprecated in the future release of the UNSW toolkit.\n",
    "    df[\"rec_id\"] = df.index.values.tolist()\n",
    "    indices_1 = []\n",
    "    indices_2 = []\n",
    "    processed = 0\n",
    "    for match_id in df[\"match_id\"].unique():\n",
    "        if match_id != -1:    \n",
    "            processed = processed + 1\n",
    "            # print(\"In routine generate_true_links(), count =\", processed)\n",
    "            # clear_output(wait=True)\n",
    "            linkages = df.loc[df['match_id'] == match_id]\n",
    "            for j in range(len(linkages)-1):\n",
    "                for k in range(j+1, len(linkages)):\n",
    "                    indices_1 = indices_1 + [linkages.iloc[j][\"rec_id\"]]\n",
    "                    indices_2 = indices_2 + [linkages.iloc[k][\"rec_id\"]]    \n",
    "    links = pd.MultiIndex.from_arrays([indices_1,indices_2])\n",
    "    return links\n",
    "\n",
    "def generate_false_links(df, size):\n",
    "    # A counterpart of generate_true_links(), with the purpose to generate random false pairs\n",
    "    # for training. The number of false pairs in specified as \"size\".\n",
    "    df[\"rec_id\"] = df.index.values.tolist()\n",
    "    indices_1 = []\n",
    "    indices_2 = []\n",
    "    unique_match_id = df[\"match_id\"].unique()\n",
    "    unique_match_id = unique_match_id[~np.isnan(unique_match_id)] # remove nan values\n",
    "    for j in range(size):\n",
    "            false_pair_ids = choice(unique_match_id, 2)\n",
    "            candidate_1_cluster = df.loc[df['match_id'] == false_pair_ids[0]]\n",
    "            candidate_1 = candidate_1_cluster.iloc[choice(range(len(candidate_1_cluster)))]\n",
    "            candidate_2_cluster = df.loc[df['match_id'] == false_pair_ids[1]]\n",
    "            candidate_2 = candidate_2_cluster.iloc[choice(range(len(candidate_2_cluster)))]    \n",
    "            indices_1 = indices_1 + [candidate_1[\"rec_id\"]]\n",
    "            indices_2 = indices_2 + [candidate_2[\"rec_id\"]]  \n",
    "    links = pd.MultiIndex.from_arrays([indices_1,indices_2])\n",
    "    return links\n",
    "\n",
    "def swap_fields_flag(f11, f12, f21, f22):\n",
    "    return ((f11 == f22) & (f12 == f21)).astype(float)\n",
    "\n",
    "def join_names_space(f11, f12, f21, f22):\n",
    "    return ((f11+\" \"+f12 == f21) | (f11+\" \"+f12 == f22)| (f21+\" \"+f22 == f11)| (f21+\" \"+f22 == f12)).astype(float)\n",
    "\n",
    "def join_names_dash(f11, f12, f21, f22):\n",
    "    return ((f11+\"-\"+f12 == f21) | (f11+\"-\"+f12 == f22)| (f21+\"-\"+f22 == f11)| (f21+\"-\"+f22 == f12)).astype(float)\n",
    "\n",
    "def abb_surname(f1, f2):\n",
    "    return ((f1[0]==f2) | (f1==f2[0])).astype(float)\n",
    "\n",
    "def reset_day(f11, f12, f21, f22):\n",
    "    return (((f11 == 1) & (f12 == 1))|((f21 == 1) & (f22 == 1))).astype(float)\n",
    "\n",
    "def extract_features(df, links):\n",
    "    c = rl.Compare()\n",
    "    c.string('given_name', 'given_name', method='levenshtein', label='y_name_leven')\n",
    "    c.string('surname', 'surname', method='levenshtein', label='y_surname_leven')  \n",
    "    c.string('given_name', 'given_name', method='jarowinkler', label='y_name_jaro')\n",
    "    c.string('surname', 'surname', method='jarowinkler', label='y_surname_jaro')  \n",
    "    c.string('postcode', 'postcode', method='jarowinkler', label='y_postcode')      \n",
    "    exact_fields = ['postcode', 'address_1', 'address_2', 'street_number']\n",
    "    for field in exact_fields:\n",
    "        c.exact(field, field, label='y_'+field+'_exact')\n",
    "    c.compare_vectorized(reset_day,('day', 'month'), ('day', 'month'),label='reset_day_flag')    \n",
    "    c.compare_vectorized(swap_fields_flag,('day', 'month'), ('day', 'month'),label='swap_day_month')    \n",
    "    c.compare_vectorized(swap_fields_flag,('surname', 'given_name'), ('surname', 'given_name'),label='swap_names')    \n",
    "    c.compare_vectorized(join_names_space,('surname', 'given_name'), ('surname', 'given_name'),label='join_names_space')\n",
    "    c.compare_vectorized(join_names_dash,('surname', 'given_name'), ('surname', 'given_name'),label='join_names_dash')\n",
    "    c.compare_vectorized(abb_surname,'surname', 'surname',label='abb_surname')\n",
    "    # Build features\n",
    "    feature_vectors = c.compute(links, df, df)\n",
    "    return feature_vectors\n",
    "\n",
    "def generate_train_X_y(df):\n",
    "    # This routine is to generate the feature vector X and the corresponding labels y\n",
    "    # with exactly equal number of samples for both classes to train the classifier.\n",
    "    pos = extract_features(df, train_true_links)\n",
    "    train_false_links = generate_false_links(df, len(train_true_links))    \n",
    "    neg = extract_features(df, train_false_links)\n",
    "    X = pos.values.tolist() + neg.values.tolist()\n",
    "    y = [1]*len(pos)+[0]*len(neg)\n",
    "    X, y = shuffle(X, y, random_state=0)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "def train_model(modeltype, modelparam, train_vectors, train_labels, modeltype_2):\n",
    "    if modeltype == 'svm': # Support Vector Machine\n",
    "        model = svm.SVC(C = modelparam, kernel = modeltype_2)\n",
    "        model.fit(train_vectors, train_labels) \n",
    "    elif modeltype == 'lg': # Logistic Regression\n",
    "        model = LogisticRegression(C=modelparam, penalty = modeltype_2,class_weight=None, dual=False, fit_intercept=True, \n",
    "                                   intercept_scaling=1, max_iter=5000, multi_class='ovr', \n",
    "                                   n_jobs=1, random_state=None)\n",
    "        model.fit(train_vectors, train_labels)\n",
    "    elif modeltype == 'nb': # Naive Bayes\n",
    "        model = GaussianNB()\n",
    "        model.fit(train_vectors, train_labels)\n",
    "    elif modeltype == 'nn': # Neural Network\n",
    "        model = MLPClassifier(solver='lbfgs', alpha=modelparam, hidden_layer_sizes=(256, ), \n",
    "                              activation = modeltype_2,random_state=None, batch_size='auto', \n",
    "                              learning_rate='constant',  learning_rate_init=0.001, \n",
    "                              power_t=0.5, max_iter=30000, shuffle=True, \n",
    "                              tol=0.0001, verbose=False, warm_start=False, momentum=0.9, \n",
    "                              nesterovs_momentum=True, early_stopping=False, \n",
    "                              validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "        model.fit(train_vectors, train_labels)\n",
    "    return model\n",
    "\n",
    "def classify(model, test_vectors):\n",
    "    result = model.predict(test_vectors)\n",
    "    return result\n",
    "\n",
    "    \n",
    "def evaluation(test_labels, result):\n",
    "    true_pos = np.logical_and(test_labels, result)\n",
    "    count_true_pos = np.sum(true_pos)\n",
    "    true_neg = np.logical_and(np.logical_not(test_labels),np.logical_not(result))\n",
    "    count_true_neg = np.sum(true_neg)\n",
    "    false_pos = np.logical_and(np.logical_not(test_labels), result)\n",
    "    count_false_pos = np.sum(false_pos)\n",
    "    false_neg = np.logical_and(test_labels,np.logical_not(result))\n",
    "    count_false_neg = np.sum(false_neg)\n",
    "    precision = count_true_pos/(count_true_pos+count_false_pos)\n",
    "    sensitivity = count_true_pos/(count_true_pos+count_false_neg) # sensitivity = recall\n",
    "    confusion_matrix = [count_true_pos, count_false_pos, count_false_neg, count_true_neg]\n",
    "    no_links_found = np.count_nonzero(result)\n",
    "    no_false = count_false_pos + count_false_neg\n",
    "    Fscore = 2*precision*sensitivity/(precision+sensitivity)\n",
    "    metrics_result = {'no_false':no_false, 'confusion_matrix':confusion_matrix ,'precision':precision,\n",
    "                     'sensitivity':sensitivity ,'no_links':no_links_found, 'F-score': Fscore}\n",
    "    return metrics_result\n",
    "\n",
    "def blocking_performance(candidates, true_links, df):\n",
    "    count = 0\n",
    "    for candi in candidates:\n",
    "        if df.loc[candi[0]][\"match_id\"]==df.loc[candi[1]][\"match_id\"]:\n",
    "            count = count + 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import train set...\n",
      "Train set size: 14093 , number of matched pairs:  3220\n",
      "Finished building X_train, y_train\n",
      "CPU times: user 2.3 s, sys: 48.7 ms, total: 2.35 s\n",
      "Wall time: 2.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "Source: \n",
    "K. Vo, J. Jonnagaddala and S.-T. Liaw, \"Medical-Record-Linkage-Ensemble,\" 16 February 2019. [Online]. \n",
    "Available: https://github.com/ePBRN/Medical-Record-Linkage-Ensemble/.\n",
    "'''\n",
    "## TRAIN SET CONSTRUCTION\n",
    "\n",
    "# Import\n",
    "print(\"Import train set...\")\n",
    "df_train = pd.read_csv(\"Data_to_produce_ePBRN_dataset/\"+trainset+\".csv\", index_col = \"rec_id\")\n",
    "train_true_links = generate_true_links(df_train)\n",
    "print(\"Train set size:\", len(df_train), \", number of matched pairs: \", str(len(train_true_links)))\n",
    "\n",
    "# Preprocess train set\n",
    "df_train['postcode'] = df_train['postcode'].astype(str)\n",
    "\n",
    "# Final train feature vectors and labels\n",
    "X_train, y_train = generate_train_X_y(df_train)\n",
    "print(\"Finished building X_train, y_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 ePBRN Blocking Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.5 s, sys: 120 ms, total: 51.6 s\n",
      "Wall time: 51.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "Modifying the code provided by the authors to produce the results in Table 4 of the paper. \n",
    "\n",
    "Source: \n",
    "K. Vo, J. Jonnagaddala and S.-T. Liaw, \"Medical-Record-Linkage-Ensemble,\" 16 February 2019. [Online]. \n",
    "Available: https://github.com/ePBRN/Medical-Record-Linkage-Ensemble/.\n",
    "'''\n",
    "# Blocking Criteria: declare non-match of all of the below fields disagree\n",
    "# Import\n",
    "ePBRN_blocking_results = []\n",
    "df_test = pd.read_csv(\"Data_to_produce_ePBRN_dataset/\"+testset+\".csv\", index_col = \"rec_id\")\n",
    "test_true_links = generate_true_links(df_test)\n",
    "leng_test_true_links = len(test_true_links)\n",
    "total_possible_pairs = comb(len(df_test),2)\n",
    "match_pairs = leng_test_true_links\n",
    "\n",
    "blocking_fields = [\"given_name\", \"surname\", \"postcode\"]\n",
    "all_candidate_pairs = []\n",
    "for field in blocking_fields:\n",
    "    block_indexer = rl.BlockIndex(on=field)\n",
    "    candidates = block_indexer.index(df_test)\n",
    "    detects = blocking_performance(candidates, test_true_links, df_test)\n",
    "    all_candidate_pairs = candidates.union(all_candidate_pairs)\n",
    "    \n",
    "    # row 1\n",
    "    row = []\n",
    "    row.append(field)\n",
    "    row.append('nc')\n",
    "    nc = len(candidates)\n",
    "    row.append(nc)\n",
    "    ePBRN_blocking_results.append(row)\n",
    "    \n",
    "    # row 2 \n",
    "    row = []\n",
    "    row.append(field)\n",
    "    row.append('pc')\n",
    "    pc = round(detects/match_pairs*100.0, 2)\n",
    "    row.append(pc)\n",
    "    ePBRN_blocking_results.append(row)\n",
    "    \n",
    "    # row 3\n",
    "    row = []\n",
    "    row.append(field)\n",
    "    row.append('rr')\n",
    "    rr = round((1-(len(candidates)/1.0/total_possible_pairs))*100, 2)\n",
    "    row.append(rr)\n",
    "    ePBRN_blocking_results.append(row)\n",
    "\n",
    "detects = blocking_performance(all_candidate_pairs, test_true_links, df_test)\n",
    "\n",
    "# row 1\n",
    "row_all = []\n",
    "row_all.append('All')\n",
    "row_all.append('nc')\n",
    "nc = len(all_candidate_pairs)\n",
    "row_all.append(nc)\n",
    "ePBRN_blocking_results.append(row_all)\n",
    "\n",
    "# row 2\n",
    "row_all = []\n",
    "row_all.append('All')\n",
    "row_all.append('pc')\n",
    "pc = round(detects/match_pairs*100.0, 2)\n",
    "row_all.append(pc)\n",
    "ePBRN_blocking_results.append(row_all)\n",
    "\n",
    "# row 3\n",
    "row_all = []\n",
    "row_all.append('All')\n",
    "row_all.append('rr')\n",
    "rr = round((1-(len(candidates)/1.0/total_possible_pairs))*100, 2)\n",
    "row_all.append(rr)\n",
    "ePBRN_blocking_results.append(row_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 ePBRN Classification Performance Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Learners Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test set...\n",
      "Preprocess...\n",
      "Extract feature vectors...\n",
      "Count labels of y_test: Counter({0: 357301, 1: 2614})\n",
      "Finished building X_test, y_test\n",
      "CPU times: user 32.3 s, sys: 209 ms, total: 32.5 s\n",
      "Wall time: 32.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "Source: \n",
    "K. Vo, J. Jonnagaddala and S.-T. Liaw, \"Medical-Record-Linkage-Ensemble,\" 16 February 2019. [Online]. \n",
    "Available: https://github.com/ePBRN/Medical-Record-Linkage-Ensemble/.\n",
    "'''\n",
    "## TEST SET CONSTRUCTION\n",
    "\n",
    "# Preprocess test set\n",
    "print(\"Processing test set...\")\n",
    "print(\"Preprocess...\")\n",
    "df_test['postcode'] = df_test['postcode'].astype(str)\n",
    "\n",
    "# Test feature vectors and labels construction\n",
    "print(\"Extract feature vectors...\")\n",
    "df_X_test = extract_features(df_test, all_candidate_pairs)\n",
    "vectors = df_X_test.values.tolist()\n",
    "labels = [0]*len(vectors)\n",
    "feature_index = df_X_test.index\n",
    "for i in range(0, len(feature_index)):\n",
    "    if df_test.loc[feature_index[i][0]][\"match_id\"]==df_test.loc[feature_index[i][1]][\"match_id\"]:\n",
    "        labels[i] = 1\n",
    "X_test, y_test = shuffle(vectors, labels, random_state=0)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "print(\"Count labels of y_test:\",collections.Counter(y_test))\n",
    "print(\"Finished building X_test, y_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 9s, sys: 2.54 s, total: 1min 12s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "Modifying the code provided by the authors to produce the results in Table 6 of the paper. \n",
    "Used the hyperparameters as specified by Table 5 of the paper to build the models.\n",
    "\n",
    "Source: \n",
    "K. Vo, J. Jonnagaddala and S.-T. Liaw, \"Medical-Record-Linkage-Ensemble,\" 16 February 2019. [Online]. \n",
    "Available: https://github.com/ePBRN/Medical-Record-Linkage-Ensemble/.\n",
    "'''\n",
    "\n",
    "ePBRN_classification_results = [] \n",
    "\n",
    "## BASE LEARNERS CLASSIFICATION AND EVALUATION\n",
    "################# SVM ########################\n",
    "'''\n",
    "Table 5 Hyperparameters for SVM on the FEBRL dataset\n",
    "1. RBF kernel\n",
    "2. C = 0.001\n",
    "'''\n",
    "modeltype = 'svm' # choose between 'svm', 'lg', 'nn'\n",
    "modeltype_2 = 'rbf'  # 'linear' or 'rbf' for svm, 'l1' or 'l2' for lg, 'relu' or 'logistic' for nn\n",
    "modelparam = 0.001\n",
    "\n",
    "md = train_model(modeltype, modelparam, X_train, y_train, modeltype_2)\n",
    "final_result = classify(md, X_test)\n",
    "final_eval = evaluation(y_test, final_result)\n",
    "precision = final_eval['precision']\n",
    "sensitivity = final_eval['sensitivity']\n",
    "Fscore = final_eval['F-score']\n",
    "nb_false  = final_eval['no_false']\n",
    "\n",
    "row = []\n",
    "row.append('SVM')\n",
    "row.append(round(precision*100, 2))\n",
    "row.append(round(sensitivity*100, 2))\n",
    "row.append(round(Fscore*100, 2))\n",
    "row.append(nb_false)\n",
    "ePBRN_classification_results.append(row)\n",
    "\n",
    "## BASE LEARNERS CLASSIFICATION AND EVALUATION\n",
    "################# NN ########################\n",
    "'''\n",
    "Table 5 Hyperparameters for NN on the FEBRL dataset\n",
    "1. ReLu activation with a = 2000\n",
    "'''\n",
    "modeltype = 'nn' # choose between 'svm', 'lg', 'nn'\n",
    "modeltype_2 = 'relu'  # 'linear' or 'rbf' for svm, 'l1' or 'l2' for lg, 'relu' or 'logistic' for nn\n",
    "modelparam = 2000\n",
    "\n",
    "md = train_model(modeltype, modelparam, X_train, y_train, modeltype_2)\n",
    "final_result = classify(md, X_test)\n",
    "final_eval = evaluation(y_test, final_result)\n",
    "precision = final_eval['precision']\n",
    "sensitivity = final_eval['sensitivity']\n",
    "Fscore = final_eval['F-score']\n",
    "nb_false = final_eval['no_false']\n",
    "\n",
    "row = []\n",
    "row.append('NN')\n",
    "row.append(round(precision*100,2))\n",
    "row.append(round(sensitivity*100, 2))\n",
    "row.append(round(Fscore*100, 2))\n",
    "row.append(nb_false)\n",
    "ePBRN_classification_results.append(row)\n",
    "\n",
    "## BASE LEARNERS CLASSIFICATION AND EVALUATION\n",
    "################# LR ########################\n",
    "'''\n",
    "Table 5 Hyperparameters for NN on the FEBRL dataset\n",
    "1. Regularization I2\n",
    "2. C = 0.005\n",
    "'''\n",
    "modeltype = 'lg' # choose between 'svm', 'lg', 'nn'\n",
    "modeltype_2 = 'l2'  # 'linear' or 'rbf' for svm, 'l1' or 'l2' for lg, 'relu' or 'logistic' for nn\n",
    "modelparam = 0.005\n",
    "\n",
    "md = train_model(modeltype, modelparam, X_train, y_train, modeltype_2)\n",
    "final_result = classify(md, X_test)\n",
    "final_eval = evaluation(y_test, final_result)\n",
    "precision = final_eval['precision']\n",
    "sensitivity = final_eval['sensitivity']\n",
    "Fscore = final_eval['F-score']\n",
    "nb_false = final_eval['no_false']\n",
    "\n",
    "row = []\n",
    "row.append('LR')\n",
    "row.append(round(precision*100, 2))\n",
    "row.append(round(sensitivity*100, 2))\n",
    "row.append(round(Fscore*100, 2))\n",
    "row.append(nb_false)\n",
    "ePBRN_classification_results.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 55s, sys: 21.9 s, total: 11min 17s\n",
      "Wall time: 10min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "Source: \n",
    "K. Vo, J. Jonnagaddala and S.-T. Liaw, \"Medical-Record-Linkage-Ensemble,\" 16 February 2019. [Online]. \n",
    "Available: https://github.com/ePBRN/Medical-Record-Linkage-Ensemble/.\n",
    "'''\n",
    "\n",
    "## ENSEMBLE CLASSIFICATION AND EVALUATION\n",
    "\n",
    "modeltypes = ['svm', 'nn', 'lg'] \n",
    "modeltypes_2 = ['rbf', 'relu', 'l2']\n",
    "modelparams = [0.001, 2000, 0.005]\n",
    "nFold = 10\n",
    "kf = KFold(n_splits=nFold)\n",
    "model_raw_score = [0]*3\n",
    "model_binary_score = [0]*3\n",
    "model_i = 0\n",
    "for model_i in range(3):\n",
    "    modeltype = modeltypes[model_i]\n",
    "    modeltype_2 = modeltypes_2[model_i]\n",
    "    modelparam = modelparams[model_i]\n",
    "    iFold = 0\n",
    "    result_fold = [0]*nFold\n",
    "    final_eval_fold = [0]*nFold\n",
    "    for train_index, valid_index in kf.split(X_train):\n",
    "        X_train_fold = X_train[train_index]\n",
    "        y_train_fold = y_train[train_index]\n",
    "        md =  train_model(modeltype, modelparam, X_train_fold, y_train_fold, modeltype_2)\n",
    "        result_fold[iFold] = classify(md, X_test)\n",
    "        final_eval_fold[iFold] = evaluation(y_test, result_fold[iFold])\n",
    "        iFold = iFold + 1\n",
    "    bagging_raw_score = np.average(result_fold, axis=0)\n",
    "    bagging_binary_score  = np.copy(bagging_raw_score)\n",
    "    bagging_binary_score[bagging_binary_score > 0.5] = 1\n",
    "    bagging_binary_score[bagging_binary_score <= 0.5] = 0\n",
    "    bagging_eval = evaluation(y_test, bagging_binary_score)\n",
    "    if modeltype == 'svm':\n",
    "        row = []\n",
    "        row.append('SVM-bag')\n",
    "        row.append(round(bagging_eval['precision']*100, 2))\n",
    "        row.append(round(bagging_eval['sensitivity']*100, 2))\n",
    "        row.append(round(bagging_eval['F-score']*100, 2))\n",
    "        row.append(bagging_eval['no_false'])\n",
    "        ePBRN_classification_results.append(row)\n",
    "    elif modeltype == 'nn':\n",
    "        row = []\n",
    "        row.append('NN-bag')\n",
    "        row.append(round(bagging_eval['precision']*100, 2))\n",
    "        row.append(round(bagging_eval['sensitivity']*100, 2))\n",
    "        row.append(round(bagging_eval['F-score']*100, 2))\n",
    "        row.append(bagging_eval['no_false'])\n",
    "        ePBRN_classification_results.append(row)\n",
    "    else:\n",
    "        row = []\n",
    "        row.append('LR-bag')\n",
    "        row.append(round(bagging_eval['precision']*100, 2))\n",
    "        row.append(round(bagging_eval['sensitivity']*100, 2))\n",
    "        row.append(round(bagging_eval['F-score']*100, 2))\n",
    "        row.append(bagging_eval['no_false'])\n",
    "        ePBRN_classification_results.append(row)\n",
    "    model_raw_score[model_i] = bagging_raw_score\n",
    "    model_binary_score[model_i] = bagging_binary_score\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.9 ms, sys: 16.8 ms, total: 37.6 ms\n",
      "Wall time: 6.48 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "Source: \n",
    "K. Vo, J. Jonnagaddala and S.-T. Liaw, \"Medical-Record-Linkage-Ensemble,\" 16 February 2019. [Online]. \n",
    "Available: https://github.com/ePBRN/Medical-Record-Linkage-Ensemble/.\n",
    "'''\n",
    "thres = .99\n",
    "stack_raw_score = np.average(model_raw_score, axis=0)\n",
    "stack_binary_score = np.copy(stack_raw_score)\n",
    "stack_binary_score[stack_binary_score > thres] = 1\n",
    "stack_binary_score[stack_binary_score <= thres] = 0\n",
    "stacking_eval = evaluation(y_test, stack_binary_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 156 µs, sys: 133 µs, total: 289 µs\n",
      "Wall time: 42.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "row = []\n",
    "row.append('Stack+Bag')\n",
    "row.append(round(stacking_eval['precision']*100, 2))\n",
    "row.append(round(stacking_eval['sensitivity']*100, 2))\n",
    "row.append(round(stacking_eval['F-score']*100, 2))\n",
    "row.append(stacking_eval['no_false'])\n",
    "ePBRN_classification_results.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Creating the Paper’s Table 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocking_results = pd.DataFrame(FEBRL_blocking_results, \n",
    "                                columns=['Blocking Criterion', 'Measure', 'FEBRL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocking_results_ePBRN = pd.DataFrame(ePBRN_blocking_results, \n",
    "                                      columns=['Blocking Criterion', 'Measure', 'ePBRN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocking_results = blocking_results.merge(blocking_results_ePBRN, \n",
    "                                          left_on=['Blocking Criterion', 'Measure'], \n",
    "                                          right_on=['Blocking Criterion', 'Measure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Blocking Criterion</th>\n",
       "      <th>Measure</th>\n",
       "      <th>FEBRL</th>\n",
       "      <th>ePBRN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>given_name</td>\n",
       "      <td>nc</td>\n",
       "      <td>154898.00</td>\n",
       "      <td>250888.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>given_name</td>\n",
       "      <td>pc</td>\n",
       "      <td>65.74</td>\n",
       "      <td>58.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>given_name</td>\n",
       "      <td>rr</td>\n",
       "      <td>99.69</td>\n",
       "      <td>99.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>surname</td>\n",
       "      <td>nc</td>\n",
       "      <td>170843.00</td>\n",
       "      <td>32425.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>surname</td>\n",
       "      <td>pc</td>\n",
       "      <td>66.50</td>\n",
       "      <td>55.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>surname</td>\n",
       "      <td>rr</td>\n",
       "      <td>99.66</td>\n",
       "      <td>99.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>postcode</td>\n",
       "      <td>nc</td>\n",
       "      <td>53197.00</td>\n",
       "      <td>80049.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>postcode</td>\n",
       "      <td>pc</td>\n",
       "      <td>84.38</td>\n",
       "      <td>93.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>postcode</td>\n",
       "      <td>rr</td>\n",
       "      <td>99.89</td>\n",
       "      <td>99.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>All</td>\n",
       "      <td>nc</td>\n",
       "      <td>372073.00</td>\n",
       "      <td>359915.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>All</td>\n",
       "      <td>pc</td>\n",
       "      <td>97.88</td>\n",
       "      <td>97.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>All</td>\n",
       "      <td>rr</td>\n",
       "      <td>99.89</td>\n",
       "      <td>99.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Blocking Criterion Measure      FEBRL      ePBRN\n",
       "0          given_name      nc  154898.00  250888.00\n",
       "1          given_name      pc      65.74      58.41\n",
       "2          given_name      rr      99.69      99.64\n",
       "3             surname      nc  170843.00   32425.00\n",
       "4             surname      pc      66.50      55.61\n",
       "5             surname      rr      99.66      99.95\n",
       "6            postcode      nc   53197.00   80049.00\n",
       "7            postcode      pc      84.38      93.90\n",
       "8            postcode      rr      99.89      99.88\n",
       "9                 All      nc  372073.00  359915.00\n",
       "10                All      pc      97.88      97.76\n",
       "11                All      rr      99.89      99.88"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocking_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Creating the Paper’s Table 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_results_FEBRL = pd.DataFrame(FEBRL_classification_results, \n",
    "                                            columns=['Model', 'pr(%)', 're(%)', 'fs(%)', 'fc'])\n",
    "ePBRN_classification_results = pd.DataFrame(ePBRN_classification_results, \n",
    "                                            columns=['Model', 'pr(%)', 're(%)', 'fs(%)', 'fc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>pr(%)</th>\n",
       "      <th>re(%)</th>\n",
       "      <th>fs(%)</th>\n",
       "      <th>fc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM</td>\n",
       "      <td>94.80</td>\n",
       "      <td>99.73</td>\n",
       "      <td>97.20</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NN</td>\n",
       "      <td>96.71</td>\n",
       "      <td>99.63</td>\n",
       "      <td>98.15</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LR</td>\n",
       "      <td>86.58</td>\n",
       "      <td>99.82</td>\n",
       "      <td>92.73</td>\n",
       "      <td>766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM-bag</td>\n",
       "      <td>96.23</td>\n",
       "      <td>99.65</td>\n",
       "      <td>97.91</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NN-bag</td>\n",
       "      <td>96.96</td>\n",
       "      <td>99.63</td>\n",
       "      <td>98.28</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LR-bag</td>\n",
       "      <td>87.34</td>\n",
       "      <td>99.82</td>\n",
       "      <td>93.16</td>\n",
       "      <td>717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Stack+Bag</td>\n",
       "      <td>97.70</td>\n",
       "      <td>99.61</td>\n",
       "      <td>98.64</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model  pr(%)  re(%)  fs(%)   fc\n",
       "0        SVM  94.80  99.73  97.20  281\n",
       "1         NN  96.71  99.63  98.15  184\n",
       "2         LR  86.58  99.82  92.73  766\n",
       "3    SVM-bag  96.23  99.65  97.91  208\n",
       "4     NN-bag  96.96  99.63  98.28  171\n",
       "5     LR-bag  87.34  99.82  93.16  717\n",
       "6  Stack+Bag  97.70  99.61  98.64  134"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_results_FEBRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>pr(%)</th>\n",
       "      <th>re(%)</th>\n",
       "      <th>fs(%)</th>\n",
       "      <th>fc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM</td>\n",
       "      <td>32.50</td>\n",
       "      <td>99.16</td>\n",
       "      <td>48.95</td>\n",
       "      <td>5406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NN</td>\n",
       "      <td>69.82</td>\n",
       "      <td>97.36</td>\n",
       "      <td>81.32</td>\n",
       "      <td>1169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LR</td>\n",
       "      <td>60.78</td>\n",
       "      <td>97.82</td>\n",
       "      <td>74.97</td>\n",
       "      <td>1707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM-bag</td>\n",
       "      <td>38.16</td>\n",
       "      <td>98.78</td>\n",
       "      <td>55.05</td>\n",
       "      <td>4216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NN-bag</td>\n",
       "      <td>70.61</td>\n",
       "      <td>97.32</td>\n",
       "      <td>81.84</td>\n",
       "      <td>1129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LR-bag</td>\n",
       "      <td>61.41</td>\n",
       "      <td>97.82</td>\n",
       "      <td>75.45</td>\n",
       "      <td>1664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Stack+Bag</td>\n",
       "      <td>74.10</td>\n",
       "      <td>97.32</td>\n",
       "      <td>84.14</td>\n",
       "      <td>959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model  pr(%)  re(%)  fs(%)    fc\n",
       "0        SVM  32.50  99.16  48.95  5406\n",
       "1         NN  69.82  97.36  81.32  1169\n",
       "2         LR  60.78  97.82  74.97  1707\n",
       "3    SVM-bag  38.16  98.78  55.05  4216\n",
       "4     NN-bag  70.61  97.32  81.84  1129\n",
       "5     LR-bag  61.41  97.82  75.45  1664\n",
       "6  Stack+Bag  74.10  97.32  84.14   959"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ePBRN_classification_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
