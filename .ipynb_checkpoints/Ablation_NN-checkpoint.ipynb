{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "694113e4",
   "metadata": {},
   "source": [
    "# Ablation - Evaluating Varied Neural Network Configurations\n",
    "As our second ablation, we evaluated the perfomance of competing Neural Network model configurations on the FEBRL Regen. (regenerated) dataset. Specifically, the following three model configuations were prepared:\n",
    "* NN with Logistic activation function (original study\n",
    "used ’RELU’).\n",
    "* NN with ’ADAM’ solver (original study used ’lbfgs’).\n",
    "* NN with ’SGD’ (Stochastic Gradient Descent) solver.\n",
    "\n",
    "All other configuration parameters were left unchanged except those identified above. These models were then benchmarked on the FEBRL Regn. dataset dataset over 10 runs and reported the following averaged results (along with their respective standard deviations).\n",
    "\n",
    "The results of this ablation provide us with some exciting results. It can be observed that all our custom configurations of the original NN model perfom significantly better than the one used in the original study. Moreover, they even perform better than every other model in the original study, including the ensemble model claimed by the authors as the best performer. Specifically, our NN model using Adam solver performs with the highest precision (98.85%) while Logistic Activation provides the smallest number of False Counts (96.6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00b1b1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import recordlinkage as rl, pandas as pd, numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from recordlinkage.preprocessing import phonetic\n",
    "from numpy.random import choice\n",
    "import collections, numpy\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from math import comb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8022861",
   "metadata": {},
   "source": [
    "## General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa7dd817",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Source: \n",
    "K. Vo, J. Jonnagaddala and S.-T. Liaw, \"Medical-Record-Linkage-Ensemble,\" 16 February 2019. [Online]. \n",
    "Available: https://github.com/ePBRN/Medical-Record-Linkage-Ensemble/.\n",
    "'''\n",
    "def generate_true_links(df): \n",
    "    # although the match_id column is included in the original df to imply the true links,\n",
    "    # this function will create the true_link object identical to the true_links properties\n",
    "    # of recordlinkage toolkit, in order to exploit \"Compare.compute()\" from that toolkit\n",
    "    # in extract_function() for extracting features quicker.\n",
    "    # This process should be deprecated in the future release of the UNSW toolkit.\n",
    "    df[\"rec_id\"] = df.index.values.tolist()\n",
    "    indices_1 = []\n",
    "    indices_2 = []\n",
    "    processed = 0\n",
    "    for match_id in df[\"match_id\"].unique():\n",
    "        if match_id != -1:    \n",
    "            processed = processed + 1\n",
    "            # print(\"In routine generate_true_links(), count =\", processed)\n",
    "            # clear_output(wait=True)\n",
    "            linkages = df.loc[df['match_id'] == match_id]\n",
    "            for j in range(len(linkages)-1):\n",
    "                for k in range(j+1, len(linkages)):\n",
    "                    indices_1 = indices_1 + [linkages.iloc[j][\"rec_id\"]]\n",
    "                    indices_2 = indices_2 + [linkages.iloc[k][\"rec_id\"]]    \n",
    "    links = pd.MultiIndex.from_arrays([indices_1,indices_2])\n",
    "    return links\n",
    "\n",
    "def generate_false_links(df, size):\n",
    "    # A counterpart of generate_true_links(), with the purpose to generate random false pairs\n",
    "    # for training. The number of false pairs in specified as \"size\".\n",
    "    df[\"rec_id\"] = df.index.values.tolist()\n",
    "    indices_1 = []\n",
    "    indices_2 = []\n",
    "    unique_match_id = df[\"match_id\"].unique()\n",
    "    for j in range(size):\n",
    "            false_pair_ids = choice(unique_match_id, 2)\n",
    "            candidate_1_cluster = df.loc[df['match_id'] == false_pair_ids[0]]\n",
    "            candidate_1 = candidate_1_cluster.iloc[choice(range(len(candidate_1_cluster)))]\n",
    "            candidate_2_cluster = df.loc[df['match_id'] == false_pair_ids[1]]\n",
    "            candidate_2 = candidate_2_cluster.iloc[choice(range(len(candidate_2_cluster)))]    \n",
    "            indices_1 = indices_1 + [candidate_1[\"rec_id\"]]\n",
    "            indices_2 = indices_2 + [candidate_2[\"rec_id\"]]  \n",
    "    links = pd.MultiIndex.from_arrays([indices_1,indices_2])\n",
    "    return links\n",
    "\n",
    "def swap_fields_flag(f11, f12, f21, f22):\n",
    "    return int((f11 == f22) and (f12 == f21))\n",
    "\n",
    "def extract_features(df, links):\n",
    "    c = rl.Compare()\n",
    "    c.string('given_name', 'given_name', method='jarowinkler', label='y_name')\n",
    "    c.string('given_name_soundex', 'given_name_soundex', method='jarowinkler', label='y_name_soundex')\n",
    "    c.string('given_name_nysiis', 'given_name_nysiis', method='jarowinkler', label='y_name_nysiis')\n",
    "    c.string('surname', 'surname', method='jarowinkler', label='y_surname')\n",
    "    c.string('surname_soundex', 'surname_soundex', method='jarowinkler', label='y_surname_soundex')\n",
    "    c.string('surname_nysiis', 'surname_nysiis', method='jarowinkler', label='y_surname_nysiis')\n",
    "    c.exact('street_number', 'street_number', label='y_street_number')\n",
    "    c.string('address_1', 'address_1', method='levenshtein', threshold=0.7, label='y_address1')\n",
    "    c.string('address_2', 'address_2', method='levenshtein', threshold=0.7, label='y_address2')\n",
    "    c.exact('postcode', 'postcode', label='y_postcode')\n",
    "    c.exact('day', 'day', label='y_day')\n",
    "    c.exact('month', 'month', label='y_month')\n",
    "    c.exact('year', 'year', label='y_year')\n",
    "        \n",
    "    # Build features\n",
    "    feature_vectors = c.compute(links, df, df)\n",
    "    return feature_vectors\n",
    "\n",
    "def generate_train_X_y(df):\n",
    "    # This routine is to generate the feature vector X and the corresponding labels y\n",
    "    # with exactly equal number of samples for both classes to train the classifier.\n",
    "    pos = extract_features(df, train_true_links)\n",
    "    train_false_links = generate_false_links(df, len(train_true_links))    \n",
    "    neg = extract_features(df, train_false_links)\n",
    "    X = pos.values.tolist() + neg.values.tolist()\n",
    "    y = [1]*len(pos)+[0]*len(neg)\n",
    "    X, y = shuffle(X, y, random_state=0)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "def classify(model, test_vectors):\n",
    "    result = model.predict(test_vectors)\n",
    "    return result\n",
    "\n",
    "    \n",
    "def evaluation(test_labels, result):\n",
    "    true_pos = np.logical_and(test_labels, result)\n",
    "    count_true_pos = np.sum(true_pos)\n",
    "    true_neg = np.logical_and(np.logical_not(test_labels),np.logical_not(result))\n",
    "    count_true_neg = np.sum(true_neg)\n",
    "    false_pos = np.logical_and(np.logical_not(test_labels), result)\n",
    "    count_false_pos = np.sum(false_pos)\n",
    "    false_neg = np.logical_and(test_labels,np.logical_not(result))\n",
    "    count_false_neg = np.sum(false_neg)\n",
    "    precision = count_true_pos/(count_true_pos+count_false_pos)\n",
    "    sensitivity = count_true_pos/(count_true_pos+count_false_neg) # sensitivity = recall\n",
    "    confusion_matrix = [count_true_pos, count_false_pos, count_false_neg, count_true_neg]\n",
    "    no_links_found = np.count_nonzero(result)\n",
    "    no_false = count_false_pos + count_false_neg\n",
    "    Fscore = 2*precision*sensitivity/(precision+sensitivity)\n",
    "    metrics_result = {'no_false':no_false, 'confusion_matrix':confusion_matrix ,'precision':precision,\n",
    "                     'sensitivity':sensitivity ,'no_links':no_links_found, 'F-score': Fscore}\n",
    "    return metrics_result\n",
    "\n",
    "def blocking_performance(candidates, true_links, df):\n",
    "    count = 0\n",
    "    for candi in candidates:\n",
    "        if df.loc[candi[0]][\"match_id\"]==df.loc[candi[1]][\"match_id\"]:\n",
    "            count = count + 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5374d723",
   "metadata": {},
   "source": [
    "## Ablation Presets and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8afe9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn_ablation(modeltype, modelparam, train_vectors, train_labels):\n",
    "    \n",
    "    if modeltype == 'nn_original': # NN with RELU Activation as used by authors\n",
    "        model = MLPClassifier(solver='lbfgs', alpha=modelparam, hidden_layer_sizes=(256, ), \n",
    "                              activation = 'relu',random_state=None, batch_size='auto', \n",
    "                              learning_rate='constant',  learning_rate_init=0.001, \n",
    "                              power_t=0.5, max_iter=10000, shuffle=True, \n",
    "                              tol=0.0001, verbose=False, warm_start=False, momentum=0.9, \n",
    "                              nesterovs_momentum=True, early_stopping=False, \n",
    "                              validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "        model.fit(train_vectors, train_labels)\n",
    "        \n",
    "    elif modeltype == 'nn_logistic_activation': # NN with Logistic Activation\n",
    "        model = MLPClassifier(solver='lbfgs', alpha=modelparam, hidden_layer_sizes=(256, ), \n",
    "                              activation = 'logistic',random_state=None, batch_size='auto', \n",
    "                              learning_rate='constant',  learning_rate_init=0.001, \n",
    "                              power_t=0.5, max_iter=10000, shuffle=True, \n",
    "                              tol=0.0001, verbose=False, warm_start=False, momentum=0.9, \n",
    "                              nesterovs_momentum=True, early_stopping=False, \n",
    "                              validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "        model.fit(train_vectors, train_labels)\n",
    "        \n",
    "    elif modeltype == 'nn_adam': # Neural Network using ADAM Solver\n",
    "        model = MLPClassifier(solver='adam', alpha=modelparam, hidden_layer_sizes=(256, ), \n",
    "                              activation = 'relu',random_state=None, batch_size='auto', \n",
    "                              learning_rate='constant',  learning_rate_init=0.001, \n",
    "                              power_t=0.5, max_iter=10000, shuffle=True, \n",
    "                              tol=0.0001, verbose=False, warm_start=False, momentum=0.9, \n",
    "                              nesterovs_momentum=True, early_stopping=False, \n",
    "                              validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "        model.fit(train_vectors, train_labels)\n",
    "        \n",
    "    elif modeltype == 'nn_sgd': # Neural Network using SGD Solver\n",
    "        model = MLPClassifier(solver='sgd', alpha=modelparam, hidden_layer_sizes=(256, ), \n",
    "                              activation = 'relu',random_state=None, batch_size='auto', \n",
    "                              learning_rate='adaptive',  learning_rate_init=0.001, \n",
    "                              power_t=0.5, max_iter=10000, shuffle=True, \n",
    "                              tol=0.0001, verbose=False, warm_start=False, momentum=0.9, \n",
    "                              nesterovs_momentum=True, early_stopping=False, \n",
    "                              validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "        model.fit(train_vectors, train_labels)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc235787",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Source: \n",
    "K. Vo, J. Jonnagaddala and S.-T. Liaw, \"Medical-Record-Linkage-Ensemble,\" 16 February 2019. [Online]. \n",
    "Available: https://github.com/ePBRN/Medical-Record-Linkage-Ensemble/.\n",
    "'''\n",
    "trainset = 'febrl3_UNSW'\n",
    "testset = 'febrl4_UNSW'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd5f0c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import train set...\n",
      "Train set size: 5000 , number of matched pairs:  1165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/.venv/lib/python3.9/site-packages/recordlinkage/preprocessing/encoding.py:80: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  s = s.str.replace(r\"[\\-\\_\\s]\", \"\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/.venv/lib/python3.9/site-packages/recordlinkage/preprocessing/encoding.py:80: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  s = s.str.replace(r\"[\\-\\_\\s]\", \"\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/.venv/lib/python3.9/site-packages/recordlinkage/preprocessing/encoding.py:80: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  s = s.str.replace(r\"[\\-\\_\\s]\", \"\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/.venv/lib/python3.9/site-packages/recordlinkage/preprocessing/encoding.py:80: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  s = s.str.replace(r\"[\\-\\_\\s]\", \"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished building X_train, y_train\n",
      "CPU times: user 835 ms, sys: 52.6 ms, total: 887 ms\n",
      "Wall time: 883 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "Source: \n",
    "K. Vo, J. Jonnagaddala and S.-T. Liaw, \"Medical-Record-Linkage-Ensemble,\" 16 February 2019. [Online]. \n",
    "Available: https://github.com/ePBRN/Medical-Record-Linkage-Ensemble/.\n",
    "'''\n",
    "## TRAIN SET CONSTRUCTION\n",
    "\n",
    "# Import\n",
    "print(\"Import train set...\")\n",
    "df_train = pd.read_csv(trainset+\".csv\", index_col = \"rec_id\")\n",
    "train_true_links = generate_true_links(df_train)\n",
    "print(\"Train set size:\", len(df_train), \", number of matched pairs: \", str(len(train_true_links)))\n",
    "\n",
    "# Preprocess train set\n",
    "df_train['postcode'] = df_train['postcode'].astype(str)\n",
    "df_train['given_name_soundex'] = phonetic(df_train['given_name'], method='soundex')\n",
    "df_train['given_name_nysiis'] = phonetic(df_train['given_name'], method='nysiis')\n",
    "df_train['surname_soundex'] = phonetic(df_train['surname'], method='soundex')\n",
    "df_train['surname_nysiis'] = phonetic(df_train['surname'], method='nysiis')\n",
    "\n",
    "# Final train feature vectors and labels\n",
    "X_train, y_train = generate_train_X_y(df_train)\n",
    "print(\"Finished building X_train, y_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64bd1d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import test set...\n",
      "Test set size: 10000 , number of matched pairs:  5000\n",
      "BLOCKING PERFORMANCE:\n",
      "Number of pairs of matched given_name: 154898 , detected  3287 /5000 true matched pairs, missed 1713\n",
      "Number of pairs of matched surname: 170843 , detected  3325 /5000 true matched pairs, missed 1675\n",
      "Number of pairs of matched postcode: 53197 , detected  4219 /5000 true matched pairs, missed 781\n",
      "Number of pairs of at least 1 field matched: 372073 , detected  4894 /5000 true matched pairs, missed 106\n",
      "CPU times: user 54.8 s, sys: 126 ms, total: 55 s\n",
      "Wall time: 55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "Source: \n",
    "K. Vo, J. Jonnagaddala and S.-T. Liaw, \"Medical-Record-Linkage-Ensemble,\" 16 February 2019. [Online]. \n",
    "Available: https://github.com/ePBRN/Medical-Record-Linkage-Ensemble/.\n",
    "\n",
    "Code has been modified to reproduce and print Table 4 of the paper.\n",
    "'''\n",
    "# Blocking Criteria: declare non-match of all of the below fields disagree\n",
    "# Import\n",
    "print(\"Import test set...\")\n",
    "FEBRL_blocking_results = []\n",
    "df_test = pd.read_csv(testset+\".csv\", index_col = \"rec_id\")\n",
    "test_true_links = generate_true_links(df_test)\n",
    "leng_test_true_links = len(test_true_links)\n",
    "print(\"Test set size:\", len(df_test), \", number of matched pairs: \", str(leng_test_true_links))\n",
    "\n",
    "total_possible_pairs = comb(len(df_test),2)\n",
    "match_pairs = leng_test_true_links\n",
    "\n",
    "print(\"BLOCKING PERFORMANCE:\")\n",
    "blocking_fields = [\"given_name\", \"surname\", \"postcode\"]\n",
    "all_candidate_pairs = []\n",
    "for field in blocking_fields:\n",
    "    block_indexer = rl.BlockIndex(on=field)\n",
    "    candidates = block_indexer.index(df_test)\n",
    "    detects = blocking_performance(candidates, test_true_links, df_test)\n",
    "    all_candidate_pairs = candidates.union(all_candidate_pairs)\n",
    "    print(\"Number of pairs of matched \"+ field +\": \"+str(len(candidates)), \", detected \",\n",
    "         detects,'/'+ str(leng_test_true_links) + \" true matched pairs, missed \" + \n",
    "          str(leng_test_true_links-detects) )\n",
    "    # row 1\n",
    "    row = []\n",
    "    row.append(field)\n",
    "    row.append('nc')\n",
    "    nc = len(candidates)\n",
    "    row.append(nc)\n",
    "    FEBRL_blocking_results.append(row)\n",
    "    \n",
    "    # row 2 \n",
    "    row = []\n",
    "    row.append(field)\n",
    "    row.append('pc')\n",
    "    pc = round(detects/match_pairs*100.0, 2)\n",
    "    row.append(pc)\n",
    "    FEBRL_blocking_results.append(row)\n",
    "    \n",
    "    # row 3\n",
    "    row = []\n",
    "    row.append(field)\n",
    "    row.append('rr')\n",
    "    rr = round((1-(len(candidates)/1.0/total_possible_pairs))*100, 2)\n",
    "    row.append(rr)\n",
    "    FEBRL_blocking_results.append(row)\n",
    "    \n",
    "detects = blocking_performance(all_candidate_pairs, test_true_links, df_test)\n",
    "print(\"Number of pairs of at least 1 field matched: \" + str(len(all_candidate_pairs)), \", detected \",\n",
    "     detects,'/'+ str(leng_test_true_links) + \" true matched pairs, missed \" + \n",
    "          str(leng_test_true_links-detects) )\n",
    "\n",
    "#Reproducing Table 4\n",
    "# row 1\n",
    "row_all = []\n",
    "row_all.append('All')\n",
    "row_all.append('nc')\n",
    "nc = len(all_candidate_pairs)\n",
    "row_all.append(nc)\n",
    "FEBRL_blocking_results.append(row_all)\n",
    "\n",
    "# row 2\n",
    "row_all = []\n",
    "row_all.append('All')\n",
    "row_all.append('pc')\n",
    "pc = round(detects/match_pairs*100.0, 2)\n",
    "row_all.append(pc)\n",
    "FEBRL_blocking_results.append(row_all)\n",
    "\n",
    "# row 3\n",
    "row_all = []\n",
    "row_all.append('All')\n",
    "row_all.append('rr')\n",
    "rr = round((1-(len(candidates)/1.0/total_possible_pairs))*100, 2)\n",
    "row_all.append(rr)\n",
    "FEBRL_blocking_results.append(row_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dddc7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test set...\n",
      "Preprocess...\n",
      "Extract feature vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/.venv/lib/python3.9/site-packages/recordlinkage/preprocessing/encoding.py:80: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  s = s.str.replace(r\"[\\-\\_\\s]\", \"\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/.venv/lib/python3.9/site-packages/recordlinkage/preprocessing/encoding.py:80: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  s = s.str.replace(r\"[\\-\\_\\s]\", \"\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/.venv/lib/python3.9/site-packages/recordlinkage/preprocessing/encoding.py:80: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  s = s.str.replace(r\"[\\-\\_\\s]\", \"\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/.venv/lib/python3.9/site-packages/recordlinkage/preprocessing/encoding.py:80: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  s = s.str.replace(r\"[\\-\\_\\s]\", \"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count labels of y_test: Counter({0: 367179, 1: 4894})\n",
      "Finished building X_test, y_test\n",
      "CPU times: user 35.2 s, sys: 158 ms, total: 35.3 s\n",
      "Wall time: 35.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "Source: \n",
    "K. Vo, J. Jonnagaddala and S.-T. Liaw, \"Medical-Record-Linkage-Ensemble,\" 16 February 2019. [Online]. \n",
    "Available: https://github.com/ePBRN/Medical-Record-Linkage-Ensemble/.\n",
    "'''\n",
    "## TEST SET CONSTRUCTION\n",
    "\n",
    "# Preprocess test set\n",
    "print(\"Processing test set...\")\n",
    "print(\"Preprocess...\")\n",
    "df_test['postcode'] = df_test['postcode'].astype(str)\n",
    "df_test['given_name_soundex'] = phonetic(df_test['given_name'], method='soundex')\n",
    "df_test['given_name_nysiis'] = phonetic(df_test['given_name'], method='nysiis')\n",
    "df_test['surname_soundex'] = phonetic(df_test['surname'], method='soundex')\n",
    "df_test['surname_nysiis'] = phonetic(df_test['surname'], method='nysiis')\n",
    "\n",
    "# Test feature vectors and labels construction\n",
    "print(\"Extract feature vectors...\")\n",
    "df_X_test = extract_features(df_test, all_candidate_pairs)\n",
    "vectors = df_X_test.values.tolist()\n",
    "labels = [0]*len(vectors)\n",
    "feature_index = df_X_test.index\n",
    "for i in range(0, len(feature_index)):\n",
    "    if df_test.loc[feature_index[i][0]][\"match_id\"]==df_test.loc[feature_index[i][1]][\"match_id\"]:\n",
    "        labels[i] = 1\n",
    "X_test, y_test = shuffle(vectors, labels, random_state=0)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "print(\"Count labels of y_test:\",collections.Counter(y_test))\n",
    "print(\"Finished building X_test, y_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae71957",
   "metadata": {},
   "source": [
    "## Running the Ablation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f8ecc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run number:  0\n",
      "Run number:  1\n",
      "Run number:  2\n",
      "Run number:  3\n",
      "Run number:  4\n",
      "Run number:  5\n",
      "Run number:  6\n",
      "Run number:  7\n",
      "Run number:  8\n",
      "Run number:  9\n",
      "CPU times: user 3min 9s, sys: 1min 54s, total: 5min 3s\n",
      "Wall time: 43.7 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>pr(%)</th>\n",
       "      <th>re(%)</th>\n",
       "      <th>fs(%)</th>\n",
       "      <th>fc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nn_original</td>\n",
       "      <td>96.486390</td>\n",
       "      <td>99.652636</td>\n",
       "      <td>98.043951</td>\n",
       "      <td>194.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nn_logistic_activation</td>\n",
       "      <td>98.676900</td>\n",
       "      <td>99.358398</td>\n",
       "      <td>99.016475</td>\n",
       "      <td>96.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nn_sgd</td>\n",
       "      <td>98.776555</td>\n",
       "      <td>99.094810</td>\n",
       "      <td>98.935181</td>\n",
       "      <td>104.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nn_adam</td>\n",
       "      <td>98.849954</td>\n",
       "      <td>98.794442</td>\n",
       "      <td>98.820085</td>\n",
       "      <td>115.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model      pr(%)      re(%)      fs(%)     fc\n",
       "0             nn_original  96.486390  99.652636  98.043951  194.6\n",
       "1  nn_logistic_activation  98.676900  99.358398  99.016475   96.6\n",
       "2                  nn_sgd  98.776555  99.094810  98.935181  104.4\n",
       "3                 nn_adam  98.849954  98.794442  98.820085  115.5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "Modifying the code provided by the authors to produce the results in Table 6 of the paper. \n",
    "Used the hyperparameters as specified by Table 5 of the paper to build the models.\n",
    "\n",
    "Source: \n",
    "K. Vo, J. Jonnagaddala and S.-T. Liaw, \"Medical-Record-Linkage-Ensemble,\" 16 February 2019. [Online]. \n",
    "Available: https://github.com/ePBRN/Medical-Record-Linkage-Ensemble/.\n",
    "'''\n",
    "\n",
    "NN_original_pr = [] \n",
    "NN_original_re = [] \n",
    "NN_original_fs = [] \n",
    "NN_original_fc = [] \n",
    "\n",
    "NN_logistic_pr = [] \n",
    "NN_logistic_re = [] \n",
    "NN_logistic_fs = [] \n",
    "NN_logistic_fc = []\n",
    "\n",
    "NN_sgd_pr = [] \n",
    "NN_sgd_re = [] \n",
    "NN_sgd_fs = [] \n",
    "NN_sgd_fc = []\n",
    "\n",
    "NN_adam_pr = [] \n",
    "NN_adam_re = [] \n",
    "NN_adam_fs = [] \n",
    "NN_adam_fc = []\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Run number: ', i)\n",
    "    models = ['nn_original', 'nn_logistic_activation', 'nn_sgd', 'nn_adam']\n",
    "    \n",
    "    # NN ORIGINAL\n",
    "    modeltype = models[0]\n",
    "    modelparam = 100\n",
    "    #TRAIN & EVAL\n",
    "    md = train_nn_ablation(modeltype, modelparam, X_train, y_train)\n",
    "    final_result = classify(md, X_test)\n",
    "    final_eval = evaluation(y_test, final_result)\n",
    "    precision = final_eval['precision']\n",
    "    sensitivity = final_eval['sensitivity']\n",
    "    Fscore = final_eval['F-score']\n",
    "    nb_false = final_eval['no_false']\n",
    "    #ADD RESULTS\n",
    "    NN_original_pr.append(precision)\n",
    "    NN_original_re.append(sensitivity)\n",
    "    NN_original_fs.append(Fscore)\n",
    "    NN_original_fc.append(nb_false)\n",
    "    \n",
    "    # NN LOGISTIC\n",
    "    modeltype = models[1]\n",
    "    modelparam = 100\n",
    "    #TRAIN & EVAL\n",
    "    md = train_nn_ablation(modeltype, modelparam, X_train, y_train)\n",
    "    final_result = classify(md, X_test)\n",
    "    final_eval = evaluation(y_test, final_result)\n",
    "    precision = final_eval['precision']\n",
    "    sensitivity = final_eval['sensitivity']\n",
    "    Fscore = final_eval['F-score']\n",
    "    nb_false = final_eval['no_false']\n",
    "    #ADD RESULTS\n",
    "    NN_logistic_pr.append(precision)\n",
    "    NN_logistic_re.append(sensitivity)\n",
    "    NN_logistic_fs.append(Fscore)\n",
    "    NN_logistic_fc.append(nb_false)\n",
    "    \n",
    "    # NN SGD\n",
    "    modeltype = models[2]\n",
    "    modelparam = 100\n",
    "    #TRAIN & EVAL\n",
    "    md = train_nn_ablation(modeltype, modelparam, X_train, y_train)\n",
    "    final_result = classify(md, X_test)\n",
    "    final_eval = evaluation(y_test, final_result)\n",
    "    precision = final_eval['precision']\n",
    "    sensitivity = final_eval['sensitivity']\n",
    "    Fscore = final_eval['F-score']\n",
    "    nb_false = final_eval['no_false']\n",
    "    #ADD RESULTS\n",
    "    NN_sgd_pr.append(precision)\n",
    "    NN_sgd_re.append(sensitivity)\n",
    "    NN_sgd_fs.append(Fscore)\n",
    "    NN_sgd_fc.append(nb_false)\n",
    "    \n",
    "    # NN ADAM\n",
    "    modeltype = models[3]\n",
    "    modelparam = 100\n",
    "    #TRAIN & EVAL\n",
    "    md = train_nn_ablation(modeltype, modelparam, X_train, y_train)\n",
    "    final_result = classify(md, X_test)\n",
    "    final_eval = evaluation(y_test, final_result)\n",
    "    precision = final_eval['precision']\n",
    "    sensitivity = final_eval['sensitivity']\n",
    "    Fscore = final_eval['F-score']\n",
    "    nb_false = final_eval['no_false']\n",
    "    #ADD RESULTS\n",
    "    NN_adam_pr.append(precision)\n",
    "    NN_adam_re.append(sensitivity)\n",
    "    NN_adam_fs.append(Fscore)\n",
    "    NN_adam_fc.append(nb_false)\n",
    "    \n",
    "    \n",
    "pr_col_MEAN = []\n",
    "pr_col_MEAN.append(sum(NN_original_pr) / float(len(NN_original_pr)))\n",
    "pr_col_MEAN.append(sum(NN_logistic_pr) / float(len(NN_logistic_pr)))\n",
    "pr_col_MEAN.append(sum(NN_sgd_pr) / float(len(NN_sgd_pr)))\n",
    "pr_col_MEAN.append(sum(NN_adam_pr) / float(len(NN_adam_pr)))\n",
    "\n",
    "re_col_MEAN = []\n",
    "re_col_MEAN.append(sum(NN_original_re) / float(len(NN_original_re)))\n",
    "re_col_MEAN.append(sum(NN_logistic_re) / float(len(NN_logistic_re)))\n",
    "re_col_MEAN.append(sum(NN_sgd_re) / float(len(NN_sgd_re)))\n",
    "re_col_MEAN.append(sum(NN_adam_re) / float(len(NN_adam_re)))\n",
    "\n",
    "fs_col_MEAN = []\n",
    "fs_col_MEAN.append(sum(NN_original_fs) / float(len(NN_original_fs)))\n",
    "fs_col_MEAN.append(sum(NN_logistic_fs) / float(len(NN_logistic_fs)))\n",
    "fs_col_MEAN.append(sum(NN_sgd_fs) / float(len(NN_sgd_fs)))\n",
    "fs_col_MEAN.append(sum(NN_adam_fs) / float(len(NN_adam_fs)))\n",
    "\n",
    "fc_col_MEAN = []\n",
    "fc_col_MEAN.append(sum(NN_original_fc) / float(len(NN_original_fc)))\n",
    "fc_col_MEAN.append(sum(NN_logistic_fc) / float(len(NN_logistic_fc)))\n",
    "fc_col_MEAN.append(sum(NN_sgd_fc) / float(len(NN_sgd_fc)))\n",
    "fc_col_MEAN.append(sum(NN_adam_fc) / float(len(NN_adam_fc)))\n",
    "\n",
    "\n",
    "models = ['nn_original', 'nn_logistic_activation', 'nn_sgd', 'nn_adam']\n",
    "df_means = pd.DataFrame(models, columns=['Model'])\n",
    "df_means['pr(%)'] = pr_col_MEAN\n",
    "df_means['pr(%)'] = df_means['pr(%)']*100\n",
    "df_means['re(%)'] = re_col_MEAN\n",
    "df_means['re(%)'] = df_means['re(%)']*100\n",
    "df_means['fs(%)'] = fs_col_MEAN\n",
    "df_means['fs(%)'] = df_means['fs(%)']*100\n",
    "df_means['fc'] = fc_col_MEAN\n",
    "\n",
    "df_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bded21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.59 ms, sys: 316 µs, total: 7.9 ms\n",
      "Wall time: 7.72 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>pr(%)</th>\n",
       "      <th>re(%)</th>\n",
       "      <th>fs(%)</th>\n",
       "      <th>fc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nn_original</td>\n",
       "      <td>0.045341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023407</td>\n",
       "      <td>2.236068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nn_logistic_activation</td>\n",
       "      <td>0.023328</td>\n",
       "      <td>0.010010</td>\n",
       "      <td>0.012124</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nn_sgd</td>\n",
       "      <td>0.202952</td>\n",
       "      <td>0.110434</td>\n",
       "      <td>0.049790</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nn_adam</td>\n",
       "      <td>0.552481</td>\n",
       "      <td>0.387585</td>\n",
       "      <td>0.144129</td>\n",
       "      <td>14.525839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model     pr(%)     re(%)     fs(%)         fc\n",
       "0             nn_original  0.045341  0.000000  0.023407   2.236068\n",
       "1  nn_logistic_activation  0.023328  0.010010  0.012124   1.000000\n",
       "2                  nn_sgd  0.202952  0.110434  0.049790   5.000000\n",
       "3                 nn_adam  0.552481  0.387585  0.144129  14.525839"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "import statistics\n",
    "\n",
    "pr_col_STD = []\n",
    "pr_col_STD.append(statistics.pstdev(NN_original_pr))\n",
    "pr_col_STD.append(statistics.pstdev(NN_logistic_pr))\n",
    "pr_col_STD.append(statistics.pstdev(NN_sgd_pr))\n",
    "pr_col_STD.append(statistics.pstdev(NN_adam_pr))\n",
    "\n",
    "re_col_STD = []\n",
    "re_col_STD.append(statistics.pstdev(NN_original_re))\n",
    "re_col_STD.append(statistics.pstdev(NN_logistic_re))\n",
    "re_col_STD.append(statistics.pstdev(NN_sgd_re))\n",
    "re_col_STD.append(statistics.pstdev(NN_adam_re))\n",
    "\n",
    "fs_col_STD = []\n",
    "fs_col_STD.append(statistics.pstdev(NN_original_fs))\n",
    "fs_col_STD.append(statistics.pstdev(NN_logistic_fs))\n",
    "fs_col_STD.append(statistics.pstdev(NN_sgd_fs))\n",
    "fs_col_STD.append(statistics.pstdev(NN_adam_fs))\n",
    "\n",
    "fc_col_STD = []\n",
    "fc_col_STD.append(statistics.pstdev(NN_original_fc))\n",
    "fc_col_STD.append(statistics.pstdev(NN_logistic_fc))\n",
    "fc_col_STD.append(statistics.pstdev(NN_sgd_fc))\n",
    "fc_col_STD.append(statistics.pstdev(NN_adam_fc))\n",
    "\n",
    "\n",
    "df_STD = pd.DataFrame(models, columns=['Model'])\n",
    "df_STD['pr(%)'] = pr_col_STD\n",
    "df_STD['pr(%)'] = df_STD['pr(%)']*100\n",
    "df_STD['re(%)'] = re_col_STD\n",
    "df_STD['re(%)'] = df_STD['re(%)']*100\n",
    "df_STD['fs(%)'] = fs_col_STD\n",
    "df_STD['fs(%)'] = df_STD['fs(%)']*100\n",
    "df_STD['fc'] = fc_col_STD\n",
    "\n",
    "df_STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7791c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
